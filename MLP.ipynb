{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kD45KgHydVfV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from progressbar import progressbar\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score,recall_score\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "    x_train = x_train.reshape(-1, 784) / 255.\n",
    "    x_test = x_test.reshape(-1, 784) / 255.\n",
    "    y_train = to_categorical(y_train).reshape(-1,10)\n",
    "    y_test = to_categorical(y_test).reshape(-1,10)\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = load_mnist()\n",
    "features = np.concatenate((x_train,x_test),axis=0)\n",
    "labels = np.concatenate((y_train,y_test),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(X_train, X_test,y_train, n_comp):\n",
    "    pca = PCA(n_components=n_comp)\n",
    "    pca.fit(X_train,y_train)\n",
    "    transform = pca.transform(X_test)\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2)\n",
    "n_pca = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca = pca(X_train,X_train,y_train,n_pca).reshape(-1,n_pca,1)\n",
    "X_test_pca = pca(X_train,X_test,y_train,n_pca).reshape(-1,n_pca,1)\n",
    "y_train = y_train.reshape(-1,10,1)\n",
    "y_test = y_test.reshape(-1,10,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KeDOnkPGXDTR"
   },
   "outputs": [],
   "source": [
    "def generetor_matrix(rows,cols):\n",
    "    return np.int_(np.random.rand(rows,cols)*10)\n",
    "\n",
    "def add_matrix(m1,m2):\n",
    "    return m1+m2\n",
    "  \n",
    "def subtract_matrix(m1,m2):\n",
    "    return m1-m2\n",
    "\n",
    "def multiply_matrix(m1,m2):\n",
    "    return np.dot(m1,m2)\n",
    "\n",
    "def generetor_matrix_nn(rows,cols):\n",
    "    return (np.random.rand(rows,cols)*2)-1\n",
    "\n",
    "def hadamard(m1,m2):\n",
    "    return m1*m2\n",
    "\n",
    "def escalar_multiply(m1,x):\n",
    "    return m1*x\n",
    "\n",
    "def trasnpose(m1):\n",
    "    return m1.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MPL Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iaX3cLpgtNgZ"
   },
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self,i_nodes,h_nodes,o_nodes):\n",
    "        self.i_nodes = i_nodes\n",
    "        self.h_nodes = h_nodes\n",
    "        self.o_nodes = o_nodes\n",
    "\n",
    "        self.bias_ih = generetor_matrix_nn(self.h_nodes,1)\n",
    "        self.bias_ho = generetor_matrix_nn(self.o_nodes,1)\n",
    "\n",
    "        self.weigths_ih = generetor_matrix_nn(self.h_nodes,self.i_nodes)\n",
    "        self.weigths_ho = generetor_matrix_nn(self.o_nodes,self.h_nodes)\n",
    "\n",
    "        self.learning_rate = 0.01\n",
    "\n",
    "    def signoid(self,x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "  \n",
    "    def dsignoid(self,x):\n",
    "        return( x * (1-x))\n",
    "\n",
    "    def train(self,input_,expected):\n",
    "        #feedforward\n",
    "        hidden = multiply_matrix(self.weigths_ih,input_)\n",
    "        hidden = add_matrix(hidden,self.bias_ih)\n",
    "        hidden = self.signoid(hidden)\n",
    "\n",
    "        output = multiply_matrix(self.weigths_ho,hidden)\n",
    "        output = add_matrix(output,self.bias_ho)\n",
    "        output = self.signoid(output)\n",
    "\n",
    "        #backpropagation\n",
    "\n",
    "        #output to hidden\n",
    "        output_error = subtract_matrix(expected,output)\n",
    "        d_output = self.dsignoid(output)\n",
    "        hidden_t = trasnpose(hidden)\n",
    "\n",
    "        gradient = hadamard(d_output,output_error)\n",
    "        gradient = escalar_multiply(gradient,self.learning_rate)\n",
    "        #ajust Bias 0 to H\n",
    "        self.bias_ho = add_matrix(self.bias_ho,gradient)\n",
    "\n",
    "        weigths_ho_deltas = multiply_matrix(gradient,hidden_t)\n",
    "        self.weigths_ho = add_matrix(self.weigths_ho,weigths_ho_deltas)\n",
    "\n",
    "\n",
    "        #hidden to input\n",
    "        weigths_ho_t = trasnpose(self.weigths_ho)\n",
    "        hidden_error = multiply_matrix(weigths_ho_t,output_error)\n",
    "        d_hidden = self.dsignoid(hidden)\n",
    "        input_t = trasnpose(input_)\n",
    "\n",
    "        gradient_h = hadamard(hidden_error,d_hidden)\n",
    "        gradient_h = escalar_multiply(gradient_h,self.learning_rate)\n",
    "        #ajust Bias H to I\n",
    "        self.bias_ih = add_matrix(self.bias_ih,gradient_h)\n",
    "        weigths_ih_deltas = multiply_matrix(gradient_h,input_t)\n",
    "        self.weigths_ih = add_matrix(self.weigths_ih,weigths_ih_deltas)\n",
    "        \n",
    "        return self.weigths_ih, self.weigths_ho \n",
    "        \n",
    "    def predict(self,input_,expected):\n",
    "        hidden = multiply_matrix(self.weigths_ih,input_)\n",
    "        hidden = add_matrix(hidden,self.bias_ih)\n",
    "        hidden = self.signoid(hidden)\n",
    "\n",
    "        output = multiply_matrix(self.weigths_ho,hidden)\n",
    "        output = add_matrix(output,self.bias_ho)\n",
    "        output = self.signoid(output)\n",
    "        \n",
    "        return output.argmax()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "maILtdpSuCsF"
   },
   "outputs": [],
   "source": [
    "nn = NN(20,20,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16653,
     "status": "ok",
     "timestamp": 1582073355881,
     "user": {
      "displayName": "Ryan Sales UFPI",
      "photoUrl": "",
      "userId": "13938675137084485046"
     },
     "user_tz": 180
    },
    "id": "_luTR96FuHp9",
    "outputId": "27c2170d-1722-4564-8587-d42eb56099ff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (56000 of 56000) |##################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
      "100% (56000 of 56000) |##################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
      "100% (56000 of 56000) |##################| Elapsed Time: 0:00:05 Time:  0:00:05\n",
      "100% (56000 of 56000) |##################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
      "100% (56000 of 56000) |##################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
      "100% (56000 of 56000) |##################| Elapsed Time: 0:00:05 Time:  0:00:05\n",
      "100% (56000 of 56000) |##################| Elapsed Time: 0:00:05 Time:  0:00:05\n",
      "100% (56000 of 56000) |##################| Elapsed Time: 0:00:05 Time:  0:00:05\n",
      "100% (56000 of 56000) |##################| Elapsed Time: 0:00:05 Time:  0:00:05\n",
      "100% (56000 of 56000) |##################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
      "100% (56000 of 56000) |##################| Elapsed Time: 0:00:05 Time:  0:00:05\n",
      "100% (56000 of 56000) |##################| Elapsed Time: 0:00:05 Time:  0:00:05\n",
      "100% (56000 of 56000) |##################| Elapsed Time: 0:00:05 Time:  0:00:05\n",
      "100% (56000 of 56000) |##################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
      "100% (56000 of 56000) |##################| Elapsed Time: 0:00:05 Time:  0:00:05\n",
      "100% (56000 of 56000) |##################| Elapsed Time: 0:00:05 Time:  0:00:05\n",
      "100% (56000 of 56000) |##################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
      "100% (56000 of 56000) |##################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
      "100% (56000 of 56000) |##################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
      "100% (56000 of 56000) |##################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
      "100% (56000 of 56000) |##################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
      "100% (56000 of 56000) |##################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
      "100% (56000 of 56000) |##################| Elapsed Time: 0:00:05 Time:  0:00:05\n",
      "100% (56000 of 56000) |##################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
      "100% (56000 of 56000) |##################| Elapsed Time: 0:00:05 Time:  0:00:05\n",
      "100% (56000 of 56000) |##################| Elapsed Time: 0:00:05 Time:  0:00:05\n",
      "100% (56000 of 56000) |##################| Elapsed Time: 0:00:05 Time:  0:00:05\n",
      "100% (56000 of 56000) |##################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
      "100% (56000 of 56000) |##################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
      "100% (56000 of 56000) |##################| Elapsed Time: 0:00:04 Time:  0:00:04\n",
      "100% (30 of 30) |########################| Elapsed Time: 0:02:28 Time:  0:02:28\n"
     ]
    }
   ],
   "source": [
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "for i in progressbar(range(30)):\n",
    "    for j in progressbar(range(len(X_train_pca))):\n",
    "        w_ih,w_ho = nn.train(X_train_pca[j],y_train[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (14000 of 14000) |##################| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    }
   ],
   "source": [
    "for i in progressbar(range(len(X_test_pca))):\n",
    "    y_pred.append(nn.predict(X_test_pca[i],y_test[i]))\n",
    "    y_true.append(y_test[i].argmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_pred,y_true)\n",
    "acc = accuracy_score(y_pred,y_true)\n",
    "recall = recall_score(y_pred,y_true,average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1280    1   15    4    3   13   12    6   10    5]\n",
      " [   1 1529    3    8    4    6    1    4   12    1]\n",
      " [   5    8 1273   29    5    3   15   16   25   11]\n",
      " [   5   19   21 1228    1   29    1    4   41   26]\n",
      " [   1    3   12    2 1271    5    6   20   11   56]\n",
      " [  11    4    1   38    1 1126   12    7   33   10]\n",
      " [   9    3   23    8   19   21 1315    0   16    0]\n",
      " [   3    2   29   23    4    7    0 1376   11   31]\n",
      " [  11   17   30   44    9   18   15    7 1216   17]\n",
      " [   1    2    7   21   55   15    0   49   15 1238]]\n",
      "Accuracy: 0.918\n",
      "Recal: 0.9171483663579254\n"
     ]
    }
   ],
   "source": [
    "print(cm)\n",
    "print(\"Accuracy:\",acc)\n",
    "print(\"Recal:\",recall)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMmGUt3IgVuW9Snbvl5R0sG",
   "collapsed_sections": [],
   "name": "NN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
